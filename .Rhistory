left_join(wraa_pa %>% filter(Balls == 3 & Strikes == 2), by = c("YEAR", "HOME_COMPETITION_LEVEL_DESCRIPTION", "HOME_TXR_LEAGUE_ID")) %>%
left_join(woba_rv, by = c("YEAR", "HOME_TXR_LEAGUE_ID")) %>%
mutate(OrigRunValue = wRAA_perPA.x*-1,
Value_of_Ball = (RunBB - wRAA_perPA.x)*-1,
Value_of_Strike = (wRAA_perPA.y - wRAA_perPA.x)*-1) %>%
select(Balls = Balls.x, Strikes = Strikes.x, YEAR, HOME_COMPETITION_LEVEL_DESCRIPTION, HOME_TXR_LEAGUE_ID,
OrigRunValue, wRAA_perPA = wRAA_perPA.x, Value_of_Ball, Value_of_Strike)
, wraa_pa %>% filter(Balls == 3 & Strikes == 2) %>%
left_join(so_outs_all, by = c("YEAR", "HOME_COMPETITION_LEVEL_DESCRIPTION", "HOME_TXR_LEAGUE_ID")) %>%
left_join(woba_rv, by = c("YEAR", "HOME_TXR_LEAGUE_ID")) %>%
mutate(OrigRunValue = wRAA_perPA*-1,
Value_of_Ball = (RunBB - wRAA_perPA)*-1,
Value_of_Strike = (SO_RunValue - wRAA_perPA)*-1) %>%
select(Balls, Strikes, YEAR, HOME_COMPETITION_LEVEL_DESCRIPTION, HOME_TXR_LEAGUE_ID,
OrigRunValue, wRAA_perPA, Value_of_Ball, Value_of_Strike)
)
View(runvalue_ball_strike)
View(woba_lg_avg)
View(woba_lg_avg)
"
Need this script converted to python. Output should be a table for PA level run expectancy (RE24), pitch level run expectancy (RE288), and event run expectancy.
Similar to this, it should be set up to run one level/year at a time.
The tables should look like 'dbo.RunExpectancies' in sql server (to get there the output of this script needs to be pivoted)
Once the script is set up and working properly we need to work with Booth/Stoll to get this into production.
"
# setwd("C:/USERS/BMELE/Documents/GitHub/run-expectancy")
setwd("C:/repos/draft-model/")
pacman::p_load(tidyverse,odbc,RPostgreSQL,tidymodels,gt)
# conn_pg <- dbConnect(drv, dbname = "postgres",host = "35.194.54.205", port = 5432,user = "postgres",password = "P@loDur0!")
# conn_ss <- dbConnect(odbc(), Driver = "{SQL Server}", Server = "TXR-ARL-SCTTST2", Database = "scouting", Trusted_Connection = "True")
conn_sf <- dbConnect(odbc::odbc(), "Snowflake",uid="bmele@texasrangers.com", pwd="@3L3m55503", warehouse = "WH_ANALYTICS_PROD", database = "PROD_EDW_DB")
source('./data/get_re_data.R')
re24_all <- tibble()
re288_all <- tibble()
event_re_all <- tibble()
for(i in 2014:2021){
target_year <- i
min_year <- i-2
for(g in 1:8){
lg <- g
# load data and create necessary columns
all_events <- dbGetQuery(conn_sf,
str_glue("{query} and wh.year >= {min_year} and wh.year <= {target_year} and split_part(home_competition_level_id, '-', 2)  = {lg}")) %>%
rename_all(tolower) %>%
# filter out bad data
filter(outs_pre_event >= 0 & outs_pre_event <= 3 & outs_post_event <= 3 & outs_post_event >=0) %>%
group_by(game_pk,inning,is_top_of_inning) %>%
# make sure only completed innings are included
mutate(complete_inning = ifelse(min(outs_pre_event) ==0 & min(batter_app_per_inning) ==1 & max(outs_post_event) == 3 |
(inning >=9 & is_top_of_inning == F & max(home_score_post_event) > max(away_score_post_event)),1,0)) %>%
group_by(game_pk) %>%
mutate(has_dh = ifelse(sum(ifelse(batter_position == 'DH',1,0))>0,1,0)) %>%
ungroup() %>%
# account for pickoffs with pre-event balls/strikes th, create binary runner columns and clean up scoring columns
mutate(strikes_pre_event = ifelse((strikes_pre_event == 3 | balls_pre_event == 4) & is_pickoff == T,0,strikes_pre_event), #pickoffs go with the pitch number as the preceding pitch but the count is the result of that pitch. so pickoffs after a strikeout will have strikes = 3
balls_pre_event = ifelse((strikes_pre_event == 3 | balls_pre_event == 4) & is_pickoff == T,0,balls_pre_event),
runner_first = ifelse(is.na(runner_1b_txr_player_id),0,1),
runner_second = ifelse(is.na(runner_2b_txr_player_id),0,1),
runner_third = ifelse(is.na(runner_3b_txr_player_id),0,1),
is_event = ifelse(play_result != 'Not Applicable' | pitch_result == 'Hit Into Play',1,0),
home_score_post_event = ifelse(is_event == 0,NA,home_score_post_event),
away_score_post_event = ifelse(is_event == 0, NA,away_score_post_event)) %>%
# for each game make sure it is in the right sequence
group_by(game_pk) %>%
arrange(batter_per_game,pitch_number,is_pickoff,pickoff_number) %>%
mutate(sequence = row_number()) %>%
group_by(game_pk, is_event) %>%
arrange(sequence) %>%
# get the post-event score
mutate(event_no = ifelse(is_event == 0,NA,row_number())
#home_score_post_event = ifelse(batter_per_game > 1 & lag(coalesce(home_score_post_event,0))>home_score_post_event,lag(coalesce(home_score_post_event,0)) + ifelse(is_top_of_inning == T,0,runs_scored_count),home_score_post_event),
#away_score_post_event = ifelse(batter_per_game > 1 & lag(coalesce(away_score_post_event,0))>away_score_post_event,lag(coalesce(away_score_post_event,0)) + ifelse(is_top_of_inning == F,0,runs_scored_count),away_score_post_event)
# home_score_pre_event = home_score_post_event - ifelse(is_top_of_inning == T,0,runs_scored_count),
# away_score_pre_event = away_score_post_event - ifelse(is_top_of_inning == F,0,runs_scored_count)
) %>%
# fill in any missing score columns with the score from the last event
group_by(game_pk,batter_per_game) %>%
arrange(sequence) %>%
fill(home_score_pre_event, away_score_pre_event, .direction = "up") %>%
ungroup() %>%
# to calculate RE:
# (1) group by each inning
# (2) find the score at the start of the inning
# (3) find the total runs scored in the inning
# (4) for each event in the inning, calculate the future runs scored from before that event to the end of the inning
# (5) group by the desired states and average the future runs scored
group_by(game_pk,inning,is_top_of_inning) %>%
mutate(home_score_post_event = coalesce(home_score_post_event,home_score_pre_event + ifelse(is_top_of_inning == T,0,runs_scored_count)),
away_score_post_event = coalesce(away_score_post_event,away_score_pre_event + ifelse(is_top_of_inning == F,0,runs_scored_count))) %>%
fill(home_score_post_event, home_score_pre_event,away_score_pre_event,away_score_post_event, .direction = "up") %>%
mutate(
score_start_home = min(home_score_pre_event, na.rm = T),
score_start_away = min(away_score_pre_event, na.rm = T),
home_team_score_diff = (home_score_pre_event - away_score_pre_event),
runs_scored_inning = max(ifelse(is_top_of_inning == 0,home_score_post_event,away_score_post_event), na.rm = T) - min(ifelse(is_top_of_inning == 0,home_score_pre_event,away_score_pre_event), na.rm = T),
future_runs = runs_scored_inning - ifelse(is_top_of_inning == 1,away_score_pre_event - score_start_away,home_score_pre_event - score_start_home)) %>%
ungroup()
# Base/Out states (RE24)
# issue here is sample size of count/events
re24 <- all_events %>%
filter(year == target_year) %>%
filter(complete_inning == 1) %>%
group_by(year,base_state, outs_pre_event) %>%
summarise(re = mean(future_runs, na.rm = T))
# Base/Out/Count States (RE 288)
re288 <- all_events %>%
mutate(wt = ifelse(year == target_year,4,
ifelse(year ==target_year -1,2,1))) %>%
filter(complete_inning == 1) %>%
group_by(base_state, outs_pre_event, count) %>%
summarise(re = stats::weighted.mean(future_runs,w=wt, na.rm=T),
n  = n()
)
# Average run value of PA ending events
event_delta_24 <- all_events %>%
mutate(wt = ifelse(year == target_year,1,0)) %>%
filter(complete_inning == 1 & !is.na(play_result) & is_plate_appearance == 1) %>%
mutate(
event = case_when(
play_result %in% c("Fielder's Choice Out","Triple Play","Out","Sac Fly (DP)","Sac Fly", "Groundout (DP)","Triple Play","Double Play") ~ 'BIP_Out',
play_result %in% c("Strikeout","Stikeout (DP)") ~ 'SO',
play_result %in% c("Walk") ~ 'BB',
play_result %in% c("Single") ~ '1B',
play_result %in% c("Double") ~ '2B',
play_result %in% c("Triple") ~ '3B',
play_result %in% c("Home Run") ~ 'HR',
play_result %in% c("Hit By Pitch") ~ 'HBP'
)
) %>% filter(!is.na(event) & is_plate_appearance == 1) %>%
left_join(
re24
) %>%
group_by(game_pk,inning,half_inning) %>%
arrange(pitch_per_game) %>%
mutate(re_post = (coalesce(lead(re),0)) + coalesce(runs_scored_count,0),
delta_runs = (coalesce(lead(re),0) - coalesce(re,0)) + coalesce(runs_scored_count,0)) %>%
#filter(game_pk == 565415)
group_by(base_state,outs_pre_event,event) %>%
summarise(re_post_event = stats::weighted.mean(re_post,w=wt,na.rm = T),
delta_runs = stats::weighted.mean(delta_runs,w = wt,na.rm = T))
event_delta_24$year <- target_year
event_delta_24$league_group_id <- lg
re288$year <- target_year
re288$league_group_id <- lg
re24$league_group_id <- lg
re288_all <- bind_rows(re288_all,
re288)
re24_all <- bind_rows(re24_all,
re24)
event_re_all <- bind_rows(event_re_all,
event_delta_24)
}
}
shiny::runApp('C:/Users/abooth/source/repos/shiny-server/pd_hitter_training')
runApp('C:/Users/abooth/source/repos/shiny-server/pd_hitter_training')
runApp('C:/Users/abooth/source/repos/shiny-server/pd_hitter_training')
shiny::runApp('C:/Users/abooth/source/repos/shiny-server/pd_matrix')
runApp('C:/Users/abooth/source/repos/shiny-server/pd_matrix')
install.packages("aws.s3")
library(aws.s3)
#install.packages("aws.s3")
library(aws.s3)
Sys.setenv(
"AWS_ACCESS_KEY_ID" = "AKIAR3YMGCQ2LQDKRKXX",
"AWS_SECRET_ACCESS_KEY" = "	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",
"AWS_DEFAULT_REGION" = "us-east-2"
)
bucketlist()
write.csv(iris, "iris.csv")
# Upload files to S3 bucket
put_object(
file = "iris.csv",
object = "iris.csv",
bucket = "test_test_R/v1/"
)
Sys.setenv(
"AWS_ACCESS_KEY_ID" = "AKIAR3YMGCQ2LQDKRKXX",
"AWS_SECRET_ACCESS_KEY" = "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",
"AWS_DEFAULT_REGION" = "us-east-2"
)
write.csv(iris, "iris.csv")
# Upload files to S3 bucket
put_object(
file = "iris.csv",
object = "iris.csv",
bucket = "test_test_R/v1/"
)
bucketlist()
#install.packages("aws.s3")
library(aws.s3)
ACCESS_KEY = "AKIAR3YMGCQ2LQDKRKXX"
SECRET_KEY = "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"
BUCKET = "txr-bbsystems-analytics-stage"
Sys.setenv(
"AWS_ACCESS_KEY_ID" = ACCESS_KEY,
"AWS_SECRET_ACCESS_KEY" = SECRET_KEY,
"AWS_DEFAULT_REGION" = "us-east-2"
)
bucketlist()
write.csv(iris, "iris.csv")
# Upload files to S3 bucket
put_object(
file = "iris.csv",
object = "iris.csv",
bucket = BUCKET + "/test_test_R/v1/"
)
#install.packages("aws.s3")
library(aws.s3)
ACCESS_KEY = "AKIAR3YMGCQ2LQDKRKXX"
SECRET_KEY = "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"
BUCKET = "txr-bbsystems-analytics-stage"
Sys.setenv(
"AWS_ACCESS_KEY_ID" = ACCESS_KEY,
"AWS_SECRET_ACCESS_KEY" = SECRET_KEY,
"AWS_DEFAULT_REGION" = "us-east-2"
)
bucketlist()
write.csv(iris, "iris.csv")
# Upload files to S3 bucket
put_object(
file = "iris.csv",
object = "iris.csv",
bucket = paste0(BUCKET, "/test_test_R/v1/")
)
# Upload files to S3 bucket
put_object(
file = "iris.csv",
object = "iris.csv",
bucket = paste0(BUCKET, "/test_test_R/v1")
)
shiny::runApp('C:/Users/abooth/source/repos/shiny-server/pd_matrix')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
call.centers.tb <-
tibble(
City = c(
"Dhaka",
"Franklin",
"Dublin",
"Stuttgart"
),
Country = c(
"Bangladesh",
"United States",
"Ireland",
"Deutschland"
),
Population = c(
14400000,
74794,
527612,
612441
),
Square.Miles = c(
118.3,
30.12,
44.4,
80.06
),
Population.Density = Population/Square.Miles,
Employees.Needed = Population.Density/1000
)
View(call.centers.tb)
call.centers.tb
market.research.tb <- tribble(
~Group, ~Interest, ~Show.Interest.Types, ~Age.Range, ~Retention,
"A", "Low", c("animated", "comedy", "drama"), "14-17", 0.12,
"B", "High", c("action", "suspenseful", "edgy"), "18-35", 0.87,
"C", "Medium", c("current events", "reality", "crime", "mystery"), "36-65", 0.37,
"D", "Low", c("current events", "crime", "thriller"), "66-99", 0.01
)
market.research.tb
simple.data.tb <-
read_csv("dataSet.csv")
View(simple.data.tb)
simple.data.tb
names(simple.data.tb)
mean(simple.data.tb$Amount)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
library(tidyverse)
kid.tb <- tibble(
name = c('George', 'Martha', 'John', 'Abigail'),
current.age = c(12, 11, 10, 7),
year.left = 25 - current.age
)
kid.tb
reliable_cars.tb <- tribble(
~Brand, ~Least.Reliable.Model, ~Avg.Reliability.Score,
"Toyota", "Tacoma", "80",
"Lexus", "GX", "77",
"Kia", "Sportage", "71",
"Audi", "A7", "68",
"BMW", "i3", "62"
)
reliable_cars.tb
kid.tb
kid.tb <- add_row(kid.tb, name='Thomas', current.age = 15, year.left= 25-current_age)
kid.tb <- add_row(kid.tb, name='Thomas', current.age = 15, year.left= 25-current.age)
kid.tb
reliable_cars.tb <- reliable_cars.tb %>% add_colum(Rank=1:5)
reliable_cars.tb <- reliable_cars.tb %>% add_column(Rank=1:5)
reliable_cars.tb
kid.tb <- kid.tb %>% add_row(name='Ed', current.age = 6, year.left= 25-current.age)
kid.tb
reliable_cars.tb[c(4,1,2,3)]
reliable_cars.tb <- reliable_cars.tb[c(4,1,2,3)]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
toFahrenheit <- function(temperature) {
fahrenheit <- temperature * 1.8 + 32
return(fahrenheit)
}
celsius <- c(0, -2, 0, 5.2, 5.4)
fahrenheit
fahrenheit <- numeric()
for (i in 1:length(celsius)) {
temperature_fahrenheit <- toFahrenheit(celsius[i])
fahrenheit[i] <- temperature_fahrenheit
}
fahrenheit
fahrenheit2 <- sapply(celsius, toFahrenheit)
fahrenheit2
fahrenheit3 <- lapply(celsius, toFahrenheit)
fahrenheit3
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
students <-read_csv("students.csv")
students <-read_csv("../Resources/students.csv")
schools <- read_csv("../Resources/schools.csv")
View(schools)
students %>% head()
schools %>% head()
data2 = left_join(students, schools, by=c("school_name"))
data2 %>% head()
View(data2)
View(schools)
school_count <- students$school_name %>%
unique() %>%
length()
school_count
student_count <-  students %>% nrow()
student_count
mean_reading_score <- students %>% summarize(mean(reading_score))
mean_math_score <- students %>% summarize(mean(math_score))
View(mean_math_score)
View(mean_reading_score)
percentage_passing_reading <- students %>%
filter(reading_score > 70) %>%
(nrow() * 100) / student_count %>%
round(2)
percentage_passing_reading <- students %>%
filter(reading_score > 70) %>%
nrow() * 100 / student_count %>%
round(2)
percentage_passing_reading
percentage_passing_math <-  students %>%
filter(math_score > 70) %>%
nrow() * 100 / student_count %>%
round(2)
percentage_passing_math
overall_passing_rate <- (percentage_passing_math + percentage_passing_reading) / 2
overall_passing_rate
students %>%
group_by(school_name) %>%
summarize(avg.reading=mean(reading_score), avg.math=mean(math_score))
students %>%
group_by(school_name, grade) %>%
summarize(avg.reading=mean(reading_score), avg.math=mean(math_score))
total_budget <- schools %>%
summarize(sum(budget))
View(total_budget)
paste("School count: ", school_count)
paste("Student count: ", student_count)
paste("Total budget: ", total_budget)
paste("Average reading score: ", mean_reading_score)
paste("Average math score: ", mean_math_score)
paste("% passing reading: ", percentage_passing_reading)
paste("% passing math: ", percentage_passing_math)
paste("Overall passing rate: ", overall_passing_rate)
View(mean_math_score)
# YOUR CODE HERE
total_budget %<% sapply(as.numeric)
# YOUR CODE HERE
total_budget %>% sapply(as.numeric)
# YOUR CODE HERE
total_budget <- total_budget %>% sapply(as.numeric)
# YOUR CODE HERE
total_budget <- total_budget %>% sapply(as.numeric)
mean_reading_score <- mean_reading_score %>% sapply(as.numeric)
mean_math_score <- mean_math_score %>% sapply(as.numeric)
# YOUR CODE HERE
district_summary <- tribble(
~Total.Schools, ~Total.Students, ~Total.Budget, ~Avg.Math, ~Avg.Reading, ~Percent.Passing.Math, ~Percent.Passing.Reading, ~Overall.Passing,
school_count, student_count, total_budget[[1]], mean_math_score[[1]], mean_reading_score[[1]], percentage_passing_reading, percentage_passing_math[[1]], overall_passing_rate
)
# YOUR CODE HERE
district_summary
# YOUR CODE HERE
district_summary <- tribble(
~Total.Schools, ~Total.Students, ~Total.Budget, ~Avg.Math, ~Avg.Reading, ~Percent.Passing.Math, ~Percent.Passing.Reading, ~Overall.Passing,
school_count, student_count, total_budget[[1]], mean_math_score[[1]], mean_reading_score[[1]], percentage_passing_reading, percentage_passing_math, overall_passing_rate
)
# YOUR CODE HERE
district_summary <- tribble(
~Total.Schools, ~Total.Students, ~Total.Budget, ~Avg.Math, ~Avg.Reading, ~Percent.Passing.Math, ~Percent.Passing.Reading, ~Overall.Passing,
school_count, student_count, total_budget[[1]], mean_math_score[[1]], mean_reading_score[[1]], percentage_passing_math, percentage_passing_reading, overall_passing_rate
)
# YOUR CODE HERE
district_summary
# YOUR CODE HERE
data2 %<%
group_by(type, school_name) %<%
summarise(Avg.Reading.Score = mean(reading_score))
data2 %>%
group_by(type, school_name) %>%
summarise(Avg.Reading.Score = mean(reading_score))
# YOUR CODE HERE
data2 %>%
group_by(type, school_name) %>%
summarise(Avg.Reading.Score = mean(reading_score),
Avg.Math.Score = mean(math_score),
Total_Students = n(),
Budget = max(budget),
Per.Student.Budget = max(budget) / n())
# YOUR CODE HERE
school.summary.tb <- data2 %>%
group_by(type, school_name) %>%
summarise(Avg.Reading.Score = mean(reading_score),
Avg.Math.Score = mean(math_score),
Total_Students = n(),
Budget = max(budget),
Per.Student.Budget = max(budget) / n())
school.summary.tb
school.summary.tb %>% arrange(Per.Student.Budget)
school.summary.tb %>% arrange(Per.Student.Budget) %<% desc()
school.summary.tb %>% arrange(Per.Student.Budget) %>% desc()
school.summary.tb %>% arrange(Per.Student.Budget)
head(df2)
# Suspensions
df2 <- read.csv("Suspension_coil.csv", stringsAsFactors = FALSE, check.names = FALSE)
# Suspensions
df2 <- read.csv("Suspension_Coil.csv", stringsAsFactors = FALSE, check.names = FALSE)
library(tidyverse)
setwd("C:\\Users\\abooth\\source\\smu_new\\UofM-VIRT-DATA-PT-03-2022-U-B\\01-Assignments\\15-Statistics_R\\Resources")
df <- read.csv("MechaCar_mpg.csv", stringsAsFactors = FALSE, check.names = FALSE)
head(df)
# linear regression
model = lm(mpg ~ vehicle_length + vehicle_weight + spoiler_angle + ground_clearance + AWD, data = df)
summary(model)
# Suspensions
df2 <- read.csv("Suspension_Coil.csv", stringsAsFactors = FALSE, check.names = FALSE)
head(df2)
total_summary <- df2 %>%
summarize(Mean=mean(PSI))
total_summary
total_summary <- df2 %>%
summarize(Mean=mean(PSI),
Median = median(PSI),
VAR = var(PSI),
SD= sd(PSI))
total_summary
lot_summary <- df2 %>%
group_by(Manufacturing_Lot) %>%
summarize(Mean=mean(PSI),
Median = median(PSI),
VAR = var(PSI),
SD= sd(PSI),
.groups='keep')
lot_summary
#  T Test
t.test(df2$PSI, mu=1500)
subset(df2, Manufacturing_Lot=="Lot1")
t.test(lot1$PSI, mu=1500)
lot1 = subset(df2, Manufacturing_Lot=="Lot1")
lot1 = subset(df2, Manufacturing_Lot=="Lot1")
lot2 = subset(df2, Manufacturing_Lot=="Lot2")
lot3 = subset(df2, Manufacturing_Lot=="Lot3")
t.test(lot1$PSI, mu=1500)
t.test(lot2$PSI, mu=1500)
t.test(lot3$PSI, mu=1500)
setwd("C:/Users/Choua Vang/Desktop/BootCamp-DataAnalytics-and-Visual/Class lessons-and-Homework/Homework Submissions/MechaCar_Statistical_Analysis")
lm(vehicle_length)
lm(vehicle_length + mpg, data = MechaCar_mpg)
lm(vehicle_lengths + mpg,MechaCar_mpg)
head(MechaCar_mpg)
library(dplyr)
library(jsonlite)
library(tidyverse)
MechaCar_mpg <- read.csv(file='MechaCar_mpg.csv', header = TRUE, check.names=F,stringsAsFactors = F)
MechaCar_mpg
head(MechaCar_mpg)
MechaCar_mpg_2 =lm(mpg ~ vehicle_length + vehicle_weight + spoiler_angle + AWD + ground_clearance, data=MechaCar_mpg)
lm(vehicle_length)
model = lm(mpg ~ vehicle_length + vehicle_weight + spoiler_angle + ground_clearance + AWD, data = df)
summary(model)
setwd("C:\\Users\\abooth\\source\\smu_new\\UofM-VIRT-DATA-PT-03-2022-U-B\\01-Assignments\\15-Statistics_R\\Resources")
df <- read.csv("MechaCar_mpg.csv", stringsAsFactors = FALSE, check.names = FALSE)
head(df)
# linear regression
model = lm(mpg ~ vehicle_length + vehicle_weight + spoiler_angle + ground_clearance + AWD, data = df)
summary(model)
MechaCar_mpg_2 = lm(mpg ~ vehicle_length + vehicle_weight + spoiler_angle + AWD + ground_clearance, data=MechaCar_mpg)
summary(MechaCar_mpg_2)
library(dplyr)
library(dplyr)
library(jsonlite)
library(tidyverse)
MechaCar_mpg <- read.csv(file='MechaCar_mpg.csv', header = TRUE, check.names=F,stringsAsFactors = F)
MechaCar_mpg
head(MechaCar_mpg)
MechaCar_mpg_2 = lm(mpg ~ vehicle_length + vehicle_weight + spoiler_angle + AWD + ground_clearance, data=MechaCar_mpg)
summary(MechaCar_mpg_2)
